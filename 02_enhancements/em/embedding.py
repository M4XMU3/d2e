# In the provided script, the data from normal_easy_pairs.csv is used to create a list of Document objects, which is then used to build a vector store (db) with FAISS. The embeddings from this vector store are used to perform a similarity search when calling retrieve_info(query).
# When you generate a response using the ChatGPT model through the LLMChain, the retrieve_info function is called with the message as the query. The result of the similarity search (similar_response) is then used in some way in the LLMChain to influence the generation of the response.
# Here are some key points:
# * Vectorization of Sales Response Data: The content from normal_easy_pairs.csv is used to create Document objects, and their embeddings are used to build a vector store (db). This store contains vectors representing the content of the documents.
# * Similarity Search: When you call retrieve_info(message), it performs a similarity search using the vectorized content from normal_easy_pairs.csv. The idea is to find the most similar content in terms of embeddings to the given message.
# * Influence on ChatGPT: The result of the similarity search (similar_response) is then somehow used in the LLMChain to influence the response generated by ChatGPT. This is typically done to guide the language model to generate responses that are similar to the content in normal_easy_pairs.csv.
# To be absolutely sure about how the data influences the response, you would need to inspect the code inside the LLMChain and the retrieve_info function. Look for how the similarity search results are used in the generation process.
# In the provided code, the specifics of how the similarity results influence the generation are not explicitly shown. You may need to check the documentation or implementation details of the LLMChain and the related classes (ChatOpenAI, PromptTemplate, etc.) for a more detailed understanding.
######
# For translation tasks, especially when dealing with paired data, the first approach (embedding normal and easy language pairs separately) is more straightforward and aligns better with the nature of translation. You can use the similarity search results to find the most similar easy language text given a new normal language text.
# In summary, it's generally better to embed normal and easy language pairs separately for translation tasks, as it provides a clearer and more intuitive way to find corresponding translations.


import sys
import os
import pandas as pd
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
# from langchain_community.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain

# 0. get input data
load_dotenv()
message = str(sys.argv[1])
file_path='em_translations.csv'


# 1. Vectorize the homepages in the "pages" folder
pages_folder = "pages"
normal_pages = [os.path.join(pages_folder, file) for file in os.listdir(pages_folder) if file.endswith(".txt")]
easy_pages = [os.path.join(pages_folder, file) for file in os.listdir(pages_folder) if file.endswith("-ls.txt")]

# Create a list of objects with a 'page_content' and 'metadata' attribute
class Document:
    def __init__(self, page_content, metadata=None):
        self.page_content = page_content
        self.metadata = metadata if metadata is not None else {}

# Instantiate Document objects with metadata
documents_normal = [Document(open(file).read()) for file in normal_pages]
documents_easy = [Document(open(file).read()) for file in easy_pages]

# initialize OpenAI embeddings
embeddings = OpenAIEmbeddings()
db_normal = FAISS.from_documents(documents_normal, embeddings)
db_easy = FAISS.from_documents(documents_easy, embeddings)

# 2. Function for similarity search/ retrieve translation
def retrieve_info(query):
    # low k-value to decreased computational demands
    similar_response = db_easy.similarity_search(query, k=1)
    page_contents_array = [doc.page_content for doc in similar_response]
    return page_contents_array

# 3. Setup LLMChain & prompts
llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-1106")
# template = "Bitte in leichte Sprache übersetzen: {message}"
template = "Diese Beispielübersetzung kommt einer Übersetzung des Eingabetextes in leichte Sprache am nähesten: '{example_translation}'. Bitte übersetzen Sie den folgenden Eingabetext entsprechend in leichte Sprache: '{message}'"
# Explicitly define input_variables in PromptTemplate
prompt = PromptTemplate(input_variables=["message", "example_translation"], template=template)

chain = LLMChain(llm=llm, prompt=prompt)

# 4. Retrieval augmented generation
def generate_response(message):
    retrieve_info_output = retrieve_info(message)
    # Provide the 'input' argument explicitly to the invoke method
    # Here, {Best_practice} is the retrieved information that guides the model in generating the response
    response = chain.invoke(input={'message': message, 'example_translation': retrieve_info_output[0]})
    return response


# Call the function with the input message
translation=generate_response(message)['text'].replace('\n', '\\n')

# print(message)
print("translation:", translation)

if translation:
    with open(file_path, 'a') as file:
        print(f"{translation}", file=file)
else:
        print('error in translating', file=file)
